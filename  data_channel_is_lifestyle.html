Project 3
================
Shyam & Kamlesh
11/9/2022

# Library

``` r
library(dplyr)
library(readr)
library(ggplot2)
library(tidyr)
library(DT)
```

# Introdcution

We are living in a digital world where we are more concerned about our
digital footprint and people often look toward the like and share they
get on a post that they publish online. We have several social media
websites and print media to share our articles. Online print media
platform like [Mashable](www.mashable.com) publishes thousand of online
media everyday and it is important for them get a more user engagement
from the the article they post online. In this dataset we are trying to
predict the share based on several predictor columns such as text
sentiment polarity, rate of negative words, rate of positive words, and
many more. Why this analysis is important ? From this predictive model,
Mashable can predict the number of share they can receive based on the
type of article they are publishing online.

# Data

The dataset summarizes a heterogeneous set of features about articles
published by in a period of two years.The dataset has 39644 data points
and 61 feature columns. The project is aimed to subset the original
dataset based on type of data channel (one of the six type) and then to
predict the number of shares. For our project we are using *TECH* data
channel for training and building a predictive modeling and eventually
extending same model to other five data channel. User will have
flexibility to choose the type of analysis they want based on their
personal choice.

``` r
newspopData <- readr::read_csv('OnlineNewsPopularity.csv',
                              locale = locale(encoding = 'latin1'))
# select specified data channel and drop other data channel columns
# this should be replaced by param$data_channel
newspopData <- newspopData %>% 
  filter(params$data_channel == 1)%>%
  select(everything(), -starts_with('data_channel_is_'))

datatable(newspopData)
```

<div id="htmlwidget-9495437d4f1b291724a6" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-9495437d4f1b291724a6">{"x":{"filter":"none","vertical":false,"data":[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>url<\/th>\n      <th>timedelta<\/th>\n      <th>n_tokens_title<\/th>\n      <th>n_tokens_content<\/th>\n      <th>n_unique_tokens<\/th>\n      <th>n_non_stop_words<\/th>\n      <th>n_non_stop_unique_tokens<\/th>\n      <th>num_hrefs<\/th>\n      <th>num_self_hrefs<\/th>\n      <th>num_imgs<\/th>\n      <th>num_videos<\/th>\n      <th>average_token_length<\/th>\n      <th>num_keywords<\/th>\n      <th>kw_min_min<\/th>\n      <th>kw_max_min<\/th>\n      <th>kw_avg_min<\/th>\n      <th>kw_min_max<\/th>\n      <th>kw_max_max<\/th>\n      <th>kw_avg_max<\/th>\n      <th>kw_min_avg<\/th>\n      <th>kw_max_avg<\/th>\n      <th>kw_avg_avg<\/th>\n      <th>self_reference_min_shares<\/th>\n      <th>self_reference_max_shares<\/th>\n      <th>self_reference_avg_sharess<\/th>\n      <th>weekday_is_monday<\/th>\n      <th>weekday_is_tuesday<\/th>\n      <th>weekday_is_wednesday<\/th>\n      <th>weekday_is_thursday<\/th>\n      <th>weekday_is_friday<\/th>\n      <th>weekday_is_saturday<\/th>\n      <th>weekday_is_sunday<\/th>\n      <th>is_weekend<\/th>\n      <th>LDA_00<\/th>\n      <th>LDA_01<\/th>\n      <th>LDA_02<\/th>\n      <th>LDA_03<\/th>\n      <th>LDA_04<\/th>\n      <th>global_subjectivity<\/th>\n      <th>global_sentiment_polarity<\/th>\n      <th>global_rate_positive_words<\/th>\n      <th>global_rate_negative_words<\/th>\n      <th>rate_positive_words<\/th>\n      <th>rate_negative_words<\/th>\n      <th>avg_positive_polarity<\/th>\n      <th>min_positive_polarity<\/th>\n      <th>max_positive_polarity<\/th>\n      <th>avg_negative_polarity<\/th>\n      <th>min_negative_polarity<\/th>\n      <th>max_negative_polarity<\/th>\n      <th>title_subjectivity<\/th>\n      <th>title_sentiment_polarity<\/th>\n      <th>abs_title_subjectivity<\/th>\n      <th>abs_title_sentiment_polarity<\/th>\n      <th>shares<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>

# Exploratory Data Analysis

Before starting EDA, target variable need to be normalize as it has high
variance.

``` r
data("newspopData", package = "ggplot2")
newspopData$scaledShare <- scale(newspopData$shares, center = T, scale = T)

#calculating correlation index
corr.index <- round(cor(newspopData$scaledShare, newspopData$n_tokens_content),2)
# scatterplot
gg <- ggplot(newspopData, aes(x= n_tokens_content, y = scaledShare)) +
  geom_point()+
  labs(subtitle = paste('NEED TO AUTOMATE', 'Tech', sep = ' : '), # need to automate this
      y = 'Number of Share (Scaled)',
      x = 'Number of Words in article', 
      title = 'Word Count Vs Number of Share',  # replace this with the main title
      caption = 'Source: News Popularity Dataset') + 
      geom_text(x = 4000, y = 60, size = 4, label = paste0("Correlation coefficient = ", corr.index), color = 'red')
plot(gg)
```

![](data_channel_is_lifestyle_files/figure-gfm/eda-1.png)<!-- -->

We can inspect trend of Number of shares (scaled) as a function of
Number of words in the article. If the points show an upward trend, then
the article with high number of words are shared more. However, if we
see a negative trend then we can estimate that with increasing number of
words in the article, number of shares decreases. This trend can be
investigated further with the correlation coefficient.

``` r
# table for article published during week or weekend 
df <- newspopData %>%select(starts_with('weekday'), shares)
monCount  <-  sum(df$weekday_is_monday * df$shares)/1e6
tueCount  <- sum(df$weekday_is_tuesday*df$shares)/1e6
wedCount  <- sum(df$weekday_is_wednesday*df$shares)/1e6
thurCount <- sum(df$weekday_is_thursday*df$shares)/1e6
friCount  <- sum(df$weekday_is_friday*df$shares)/1e6
satCount  <- sum(df$weekday_is_saturday*df$shares)/1e6
sunCount  <- sum(df$weekday_is_sunday*df$shares)/1e6

# creating a data frame
df <- data.frame(Day <- c('Monday', 'Tuesday', 
                          'Wednesday', 'Thursday', 
                          'Friday', 'Saturday', 'Sunday'),
                 count <-c(monCount, tueCount, wedCount,  
                           thurCount, friCount,
                           satCount, sunCount ))
# plot
gg <- ggplot(df, aes(x = factor(Day), y = count)) +
  geom_bar(stat = 'identity', width = 0.5, fill = 'tomato3')+
  labs(title = 'Bar Chart',
       subtitle = paste('Number of Shares (Million) Vs Day of Week','data channel is Tech', sep = ' : '), # need to automate this too
       caption = 'Source : News Popularity Dataset',
       y = 'Total Share count in Million',
       x = 'Day of the Week') +
  theme(axis.text.x = element_text(angle = 65, vjust = 0.6))
plot(gg)
```

![](data_channel_is_lifestyle_files/figure-gfm/unnamed-chunk-1-1.png)<!-- -->
From this bar chart we can visualize the shares trend across the week.
The users engagement with the type of data Chanel (tech, entertainment,
politics) will be different across the week. Users may be more inclined
toward sharing lifestyle and entertainment news during weekend and
prefer less to share technology/science related news during same time.

``` r
# contingency table
df <- newspopData%>%
  select(shares, starts_with('weekday_is'),  global_sentiment_polarity, starts_with('global_rate_'))%>%
  pivot_longer(cols = starts_with('weekday_is_'),
                         names_to = 'Days',
                         values_to = 'count') %>% filter(count == 1) %>% select(everything(), -count)

#trying to rename the days col
#df$days <- apply(as.data.frame(df$Days), MARGIN = 2, FUN = function(x){sub(".*_is_ ", "" , x)})
datatable(df)
```

<div id="htmlwidget-6c437bb37fff59128e2c" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-6c437bb37fff59128e2c">{"x":{"filter":"none","vertical":false,"data":[[],[],[],[],[],[]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>shares<\/th>\n      <th>global_sentiment_polarity<\/th>\n      <th>global_rate_positive_words<\/th>\n      <th>global_rate_negative_words<\/th>\n      <th>Days<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[1,2,3,4]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>

``` r
par(mfrow = c(1,2))
gg <- ggplot(df, aes(x = global_rate_positive_words, y = global_sentiment_polarity)) + 
      geom_point(aes (col = Days, size = shares/1e5)) + 
      geom_smooth(aes(col = Days), method = 'lm', se = F) + 
  labs(subtitle = 'Positive Rate VS Sentiment Polarity Plot',
       x = 'Global Positive Word Rate',
       y = 'Global Sentiment Polarity',
       title = "Needc to automate",
       caption = "Source : News Popularity Dataset")

plot(gg)
```

![](data_channel_is_lifestyle_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

``` r
gg1<- ggplot(df, aes(x = global_rate_negative_words, y = global_sentiment_polarity)) + 
      geom_point(aes (col = Days, size = shares/1e5)) + 
      geom_smooth(aes(col = Days), method = 'lm', se = F) + 
  labs(subtitle = 'Positive Rate VS Sentiment Polarity Plot',
       x = 'Global Negative Word Rate',
       y = 'Global Sentiment Polarity',
       title = "Needc to automate",
       caption = "Source : News Popularity Dataset")
plot(gg1)
```

![](data_channel_is_lifestyle_files/figure-gfm/unnamed-chunk-3-2.png)<!-- -->

This plot estimates the general sentiments of users based on positive
and negative words in the content. As expected, an article with more
positive words has a positive sentiment index and more shares.
